{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Model Error:[3.45638663]\n",
      "Prediction Value:[0 0 0 0 0 0 0 1]\n",
      "True Value:[0 1 0 0 0 1 0 1]\n",
      "------------\n",
      "1000\n",
      "Model Error:[3.63389116]\n",
      "Prediction Value:[1 1 1 1 1 1 1 1]\n",
      "True Value:[0 0 1 1 1 1 1 1]\n",
      "------------\n",
      "2000\n",
      "Model Error:[3.91366595]\n",
      "Prediction Value:[0 1 0 0 1 0 0 0]\n",
      "True Value:[1 0 1 0 0 0 0 0]\n",
      "------------\n",
      "3000\n",
      "Model Error:[3.72191702]\n",
      "Prediction Value:[1 1 0 1 1 1 1 1]\n",
      "True Value:[0 1 0 0 1 1 0 1]\n",
      "------------\n",
      "4000\n",
      "Model Error:[3.5852713]\n",
      "Prediction Value:[0 0 0 0 1 0 0 0]\n",
      "True Value:[0 1 0 1 0 0 1 0]\n",
      "------------\n",
      "5000\n",
      "Model Error:[2.53352328]\n",
      "Prediction Value:[1 0 1 0 0 0 1 0]\n",
      "True Value:[1 1 0 0 0 0 1 0]\n",
      "------------\n",
      "6000\n",
      "Model Error:[0.57691441]\n",
      "Prediction Value:[0 1 0 1 0 0 0 1]\n",
      "True Value:[0 1 0 1 0 0 0 1]\n",
      "------------\n",
      "7000\n",
      "Model Error:[1.42589952]\n",
      "Prediction Value:[1 0 0 0 0 0 0 1]\n",
      "True Value:[1 0 0 0 0 0 0 1]\n",
      "------------\n",
      "8000\n",
      "Model Error:[0.47477457]\n",
      "Prediction Value:[0 0 1 1 1 0 0 0]\n",
      "True Value:[0 0 1 1 1 0 0 0]\n",
      "------------\n",
      "9000\n",
      "Model Error:[0.21595037]\n",
      "Prediction Value:[0 0 0 0 1 1 1 0]\n",
      "True Value:[0 0 0 0 1 1 1 0]\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# RNN à chaque couche cachée n'incorpore pas seulement les informations de la précédente\n",
    "# couche mais aussi les informations du pas de temps précédent de manière récursive.\n",
    "\n",
    "# Les deux seules choses dont nous avons besoin sont 'Numpy' pour faire nos calculs et 'Copy' pour copier des données\n",
    "# Ce RNN va prédire des sommes binaires\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Nous avons d'abord besoin d'une fonction sigmoïde\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    '''convertit les valeurs en probabilité'''\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def tanh(x):\n",
    "    '''ajout de la fonction d'activation tanh pour comparaison'''\n",
    "    output = np.sinh(x)/np.cosh(x)\n",
    "    return output\n",
    "\n",
    "# Maintenant, nous devons obtenir le dérivé du sigmoïde\n",
    "\n",
    "\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    '''calcule le gradient de notre sigmoïde qui est utilisé pour trouver notre erreur'''\n",
    "    '''via une méthode connue sous le nom de descente de gradient'''\n",
    "    return output * (1 - output)\n",
    "\n",
    "def tanh_output_to_derivative(output):\n",
    "    '''calcule le gradient de notre tanh qui est utilisé pour trouver notre erreur'''\n",
    "    return output * (1 - output)\n",
    "\n",
    "# génération de jeux de données d'entraînement\n",
    "interger_to_binary = {}  # Ceci est une table de recherche qui change les entiers en binaires\n",
    "binary_dimensions = 8\n",
    "\n",
    "largest_number = pow(2, binary_dimensions)\n",
    "binary = np.unpackbits( np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "for i in range(largest_number):\n",
    "    interger_to_binary[i] = binary[i]\n",
    "\n",
    "#define les variables d'entrée\n",
    "alpha = 0.1\n",
    "input_dimensions = 2\n",
    "hidden_dimensions = 16\n",
    "output_dimensions = 1\n",
    "\n",
    "\n",
    "# Initialiser les poids RNN pour ajuster les valeurs\n",
    "\n",
    "# Synapse 0 se connecte de la couche d'entrée à la couche cachée\n",
    "# donc il a deux lignes et 16 colonnes.\n",
    "synapse_0 = 2 * np.random.random((input_dimensions, hidden_dimensions)) - 1\n",
    "\n",
    "# Synapse 1 connecte le couche caché au couche de sortie\n",
    "# donc il a 16 lignes et une colonne.\n",
    "synapse_1 = 2 * np.random.random((hidden_dimensions, output_dimensions)) - 1\n",
    "\n",
    "# Synapse h est l'endroit où la magie du RNN se produit\n",
    "# il connecte la couche cachée au pas de temps précédent et au pas de temps suivant\n",
    "# au couche caché dans le pas de temps actuel\n",
    "# Donc, il a 16 lignes et 16 colonnes\n",
    "synapse_h = 2 * np.random.random((hidden_dimensions, hidden_dimensions)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0) #initialiser le tableau de dimension 2 lignes 16 col  \n",
    "synapse_1_update = np.zeros_like(synapse_1) #initialiser le tableau de dimension 16 lignes 1 col\n",
    "synapse_h_update = np.zeros_like(synapse_h) #initialiser le tableau de dimension 16 lignes 16 col\n",
    "# Logique d'entraînement\n",
    "for j in range(10000):\n",
    "\n",
    "    # générer un problème d'addition simple (a + b = c)\n",
    "    a_int = np.random.randint(largest_number / 2)  # version int\n",
    "    a = interger_to_binary[a_int]   # encodage binaire\n",
    "    b_int = np.random.randint(largest_number / 2)  # int version\n",
    "    b = interger_to_binary[b_int]   # encodage binaire\n",
    "\n",
    "    # Vrai réponse\n",
    "    c_int = a_int + b_int\n",
    "    c = interger_to_binary[c_int]\n",
    "\n",
    "    # Où nous allons stocker la supposition codée en binaire\n",
    "    d = np.zeros_like(c) #initialisation de d dimension 1 ligne  8 col\n",
    "    overallError = 0\n",
    "\n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dimensions)) \n",
    "    #print(layer_1_values)\n",
    "    # Déplacement le long des positions dans l'encodage binaire\n",
    "    for position in range(binary_dimensions):\n",
    "\n",
    "        # Générer des entrées et des sorties\n",
    "        X = np.array([[a[binary_dimensions - position - 1], b[binary_dimensions - position - 1]]])\n",
    "        y = np.array([[c[binary_dimensions - position - 1]]]).T\n",
    "          # Couche cachée 1\n",
    "        # est passé à travers les fonctions sigmoïdes en tant qu'addition des couches\n",
    "        layer_1 = sigmoid(np.dot(X, synapse_0) + np.dot(layer_1_values[-1], synapse_h))\n",
    "\n",
    "          # Couche de sortie\n",
    "        # Passe la sortie de nos couches via la fonction sigmoïde\n",
    "        # donner une prédiction\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
    "\n",
    "        # Calculez le montant que vous êtes loin de la vraie valeur (l'erreur)\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error) * sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])  # abs est (racine carrée de la somme)\n",
    "\n",
    "\n",
    "        # Décodez l'estimation pour que nous puissions l'imprimer\n",
    "        d[binary_dimensions - position - 1] = np.round(layer_2[0][0])\n",
    "\n",
    "        # Stockez le calque caché afin qu'il puisse être utilisé dans le prochain pas de temps\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "\n",
    "    future_layer_1_delta = np.zeros(hidden_dimensions)\n",
    "\n",
    "    # Ceci est la propagation arrière\n",
    "    for position in range(binary_dimensions):\n",
    "\n",
    "        X = np.array([[a[position], b[position]]])\n",
    "        layer_1 = layer_1_values[-position - 1]\n",
    "        prev_layer_1 = layer_1_values[-position - 2]\n",
    "\n",
    "        # Erreur au niveau de la couche de sortie\n",
    "        layer_2_delta = layer_2_deltas[-position - 1]\n",
    "        # erreur au niveau du couche masqué\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # Ces informations sont utilisées pour mettre à jour les fonctions de poids\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "\n",
    "        future_layer_1_delta = layer_1_delta\n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha\n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "\n",
    "    # Impression en cours\n",
    "    if(j % 1000 == 0):\n",
    "        print(j)\n",
    "        print(\"Model Error:\" + str(overallError))\n",
    "        print(\"Prediction Value:\" + str(d))\n",
    "        print(\"True Value:\" + str(c))\n",
    "        out = 0\n",
    "#         for index, x in enumerate(reversed(d)):\n",
    "#             out += x * pow(2, index)\n",
    "#         print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import operator\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "X_train = np.load(r'X_train.npy', allow_pickle=False)\n",
    "y_train = np.load(r'Y_train.npy', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize parameters\n",
    "class RNNNumpy():\n",
    "    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):\n",
    "        # assign instance variable\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # random initiate the parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. forward propagation\n",
    "\n",
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "def forward_propagation(self, x):\n",
    "    # total num of time steps, len of vector x\n",
    "    T = len(x)\n",
    "    # during forward propagation, save all hidden stages in s, S_t = U .dot x_t + W .dot s_{t-1}\n",
    "    # we also need the initial state of s, which is set to 0\n",
    "    # each time step is saved in one row in s，each row in s is s[t] which corresponding to an rnn internal loop time\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    print(s)\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    print( s[-1])\n",
    "    # output at each time step saved as o, save them for later use\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    for t in np.arange(T):\n",
    "        # we are indexing U by x[t]. it is the same as multiplying U with a one-hot vector\n",
    "        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-81a519191100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNNNumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis = 1)\n",
    "\n",
    "RNNNumpy.predict = predict\n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)\n",
    "\n",
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. calculate the loss\n",
    "'''\n",
    "the loss is defined as\n",
    "L(y, o) = -\\frac{1}{N} \\sum_{n \\in N} y_n log(o_n)\n",
    "'''\n",
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # for each sentence ...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # we only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x, y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss\n",
    "\n",
    "print(\"Expected Loss for random prediction: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. BPTT\n",
    "'''\n",
    "1. we nudge the parameters into a direction that reduces the error. the direction is given by the gradient of the loss: \\frac{\\partial L}{\\partial U}, \n",
    "\\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}\n",
    "2. we also need learning rate: which indicated how big of a step we want to make in each direction\n",
    "Q: how to optimize SGD using batching, parallelism and adaptive learning rates.\n",
    "\n",
    "RNN BPTT: because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the\n",
    "current time step, but also the previous time steps.\n",
    "'''\n",
    "\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # we will accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1   # it is y_hat - y\n",
    "    # for each output backwards ...\n",
    "    for t in np.arange(T):\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)    # at time step t, shape is word_dim * hidden_dim\n",
    "        # initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ^ 2))\n",
    "        # backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        # given time step t, go back from time step t, to t-1, t-2, ...\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print(\"Backprogation step t=%d bptt step=%d\" %(t, bptt_step))\n",
    "            dLdW += np.outer(delta_t, s[bptt_step - 1])\n",
    "            dLdU[:, x[bptt_step]] += delta_t\n",
    "            # update delta for next step\n",
    "            dleta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]^2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 gradient checking\n",
    "'''\n",
    "verify the gradient by its definition:\n",
    "\\frac{\\partial{L}}{\\partial{\\theta}} = \\lim_{h \\propto 0} \\frac{J(\\theta + h) - J(\\theta - h)}{2h}\n",
    "'''\n",
    "def gradient_check(self, x, y, h = 0.001, error_threshold = 0.01):\n",
    "    # calculate the gradient using backpropagation\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # list of all params we want to check\n",
    "    model_parameters = [\"U\", \"V\", \"W\"]\n",
    "    # gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # get the actual parameter value from model, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print(\"performing gradient check for parameter %s with size %d. \" %(pname, np.prod(parameter.shape)))\n",
    "        # iterate over each element of the parameter matrix, e.g. (0,0), (0,1)...\n",
    "        it = np.nditer(parameter, flags = ['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # estimate the gradient using (f(x+h) - f(x-h))/2h\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x], [y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x], [y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # reset parameter to the original value\n",
    "            parameter[ix] = original_value\n",
    "            # the gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate the relative error (|x - y|)/(|x|+|y|)\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # if the error is too large fail the gradient check\n",
    "            if relative_error < error_threshold:\n",
    "                print(\"Gradient check error: parameter = %s ix = %s\" %(pname, ix))\n",
    "                print(\"+h Loss: %f\" % gradplus)\n",
    "                print(\"-h Loss: %f\" % gradminus)\n",
    "                print(\"Estimated gradient: %f\" % estimated_gradient)\n",
    "                print(\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print(\"Relative error: %f\" % relative_error)\n",
    "                return\n",
    "            it.iternext()\n",
    "        print(\"Gradient check for parameter %s passed. \" %(pname))\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate = 1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. SGD implementation\n",
    "'''\n",
    "two step:\n",
    "1. calculate the gradients and perform the updates for one batch\n",
    "2. loop through the training set and adjust the learning rate\n",
    "'''\n",
    "### 4.1. perform one step of SGD\n",
    "def numpy_sgd_step(self, x, y, learning_rate):\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "RNNNumpy.sgd_step = numpy_sgd_step\n",
    "\n",
    "### 4.2. outer SGD loop\n",
    "'''\n",
    " - model: \n",
    " - X_train:\n",
    " - y_train:\n",
    " - learning_rate:\n",
    " - nepoch:\n",
    " - evaluate loss_after:\n",
    "'''\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 100, evaluate_loss_after = 5):\n",
    "    # keep track of the losses so that we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: loss after num_examples_seen=%d epoch=%d: %f\" %(time, num_examples_seen, epoch, loss))\n",
    "            # adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"setting learning rate to %f\" %(learning_rate))\n",
    "            sys.stdout.flush()\n",
    "        # for each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # one sgd step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch = 10, evaluate_loss_after = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
